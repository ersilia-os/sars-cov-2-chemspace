{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import rdkit\n",
    "from standardiser import standardise\n",
    "from rdkit import RDLogger\n",
    "import shutil\n",
    "\n",
    "RDLogger.DisableLog(\"rdApp.*\")\n",
    "\n",
    "data_dir = os.path.join(\"..\", \"data\")\n",
    "np_dir = os.path.join(data_dir, \"original\", \"NP\")\n",
    "sd_dir = os.path.join(data_dir, \"original\", \"SD\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data curation\n",
    "The NP and SD folders contain duplicated and incorrect SMILES.\n",
    "\n",
    "We will use the sd and np PCA files to extract the correct names and apply them to sort the folders into NP_curated and SD_curated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(626, 121) 626 626\n",
      "(656, 121) 656 656\n",
      "626 656\n"
     ]
    }
   ],
   "source": [
    "np = pd.read_csv(os.path.join(data_dir, \"original\", \"PCA\", \"np_pca_cleaned.csv\"))\n",
    "sd = pd.read_csv(os.path.join(data_dir, \"original\", \"PCA\", \"sd_pca_cleaned.csv\"))\n",
    "\n",
    "np_names = [name.replace('.****', '').replace('.mol', '') for name in np[\"Title\"]]\n",
    "\n",
    "sd_names = [name.replace('.****', '').replace('.mol', '') for name in sd[\"Title\"]]\n",
    "\n",
    "print(np.shape, len(np_names), len(set(np_names)))\n",
    "print(sd.shape, len(sd_names), len(set(sd_names)))\n",
    "print(len(set(np_names)-set(sd_names)), len(set(sd_names)-set(np_names)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "626 617 617\n"
     ]
    }
   ],
   "source": [
    "source_folder = os.path.join(data_dir, \"original\", 'NP')\n",
    "destination_folder = os.path.join(data_dir, \"original\", 'NP_curated')\n",
    "correct_files = []\n",
    "for file_name in os.listdir(source_folder):\n",
    "    base_name = os.path.splitext(file_name)[0]\n",
    "    if base_name in np_names:\n",
    "        correct_files.append(base_name)\n",
    "        source_path = os.path.join(source_folder, file_name)\n",
    "        destination_path = os.path.join(destination_folder, file_name)\n",
    "        shutil.copy2(source_path, destination_path)\n",
    "num_files = len(os.listdir(destination_folder))\n",
    "\n",
    "print(len(np_names),len(correct_files), num_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Ancistrotanzanine-C',\n",
       " '26,27-Dinorergosta-5,23-dien-3-ol,(3beta)',\n",
       " 'Jubanine H',\n",
       " 'Cryptobeilic-acid-C',\n",
       " '26,27-Dinorergost-5-ene-3,24-diol,(3beta)',\n",
       " 'J02_12-E',\n",
       " 'Androstan-17-one,3-ethyl-3-hydroxy,(5alpha)',\n",
       " '162727',\n",
       " 'Annonidine-F',\n",
       " 'J02_12-Z']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np_missing = list(set(np_names)-set(correct_files))\n",
    "np_missing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ancistrotanzanine C\n",
      "26,27-Dinorergosta-5,23-dien-3-ol,(3.beta)\n",
      "Cryptobeilic acid C\n",
      "26,27-Dinorergost-5-ene-3,24-diol,(3.beta)\n",
      "J02_12_E\n",
      "Androstan-17-one, 3-ethyl-3-hydroxy,(5alpha)\n",
      "Annonidine F\n",
      "J02_12_Z\n",
      "Jubanine B\n",
      "MCSJ37_0012\n",
      "626 626\n"
     ]
    }
   ],
   "source": [
    "name_changes = {'Ancistrotanzanine-C': 'Ancistrotanzanine C',\n",
    " '26,27-Dinorergosta-5,23-dien-3-ol,(3beta)':'26,27-Dinorergosta-5,23-dien-3-ol,(3.beta)',\n",
    " 'Cryptobeilic-acid-C':'Cryptobeilic acid C',\n",
    " '26,27-Dinorergost-5-ene-3,24-diol,(3beta)':'26,27-Dinorergost-5-ene-3,24-diol,(3.beta)',\n",
    " 'J02_12-E':'J02_12_E',\n",
    " 'Androstan-17-one,3-ethyl-3-hydroxy,(5alpha)':'Androstan-17-one, 3-ethyl-3-hydroxy,(5alpha)',\n",
    " 'Annonidine-F':'Annonidine F',\n",
    " 'J02_12-Z':'J02_12_Z',\n",
    " 'Jubanine H': 'Jubanine B', \n",
    " '162727': 'MCSJ37_0012'}\n",
    "\n",
    "source_folder = os.path.join(data_dir, \"original\", 'NP')\n",
    "destination_folder = os.path.join(data_dir, \"original\", 'NP_curated')\n",
    "for k,v in name_changes.items():\n",
    "    for file_name in os.listdir(source_folder):\n",
    "        base_name = os.path.splitext(file_name)[0]\n",
    "        if base_name in v:\n",
    "            print(base_name)\n",
    "            source_path = os.path.join(source_folder, file_name)\n",
    "            destination_path = os.path.join(destination_folder, file_name)\n",
    "            shutil.copy2(source_path, destination_path)\n",
    "    num_files = len(os.listdir(destination_folder))\n",
    "\n",
    "print(len(np_names), num_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Duplicate files found:\n",
      "Base name: MCSJ49_0002 - Files: MCSJ49_0002.sdf, MCSJ49_0002.mol\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "directory_path = os.path.join(data_dir, \"original\", 'NP_curated')\n",
    "file_dict = defaultdict(list)\n",
    "for filename in os.listdir(directory_path):\n",
    "    base_name = filename.replace('.mol', '').replace('.sdf', '').replace('.mol2', '')\n",
    "    file_dict[base_name].append(filename)\n",
    "duplicates = {key: value for key, value in file_dict.items() if len(value) > 1}\n",
    "if duplicates:\n",
    "    print(\"Duplicate files found:\")\n",
    "    for base_name, files in duplicates.items():\n",
    "        print(f\"Base name: {base_name} - Files: {', '.join(files)}\")\n",
    "else:\n",
    "    print(\"No duplicates found.\")\n",
    "#manually delete the .sdf file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "656 653 653\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['Orotic-acid',\n",
       " 'Taribavirin-Hydrochloride',\n",
       " 'LY411575',\n",
       " 'Chloramphenicol-succinate']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import shutil\n",
    "\n",
    "source_folder = os.path.join(data_dir, \"original\", 'SD')\n",
    "destination_folder = os.path.join(data_dir, \"original\", 'SD_curated')\n",
    "correct_files = []\n",
    "for file_name in os.listdir(source_folder):\n",
    "    base_name = os.path.splitext(file_name)[0]\n",
    "    if base_name in sd_names:\n",
    "        correct_files.append(base_name)\n",
    "        source_path = os.path.join(source_folder, file_name)\n",
    "        destination_path = os.path.join(destination_folder, file_name)\n",
    "        shutil.copy2(source_path, destination_path)\n",
    "num_files = len(os.listdir(destination_folder))\n",
    "\n",
    "print(len(sd_names),len(correct_files), num_files)\n",
    "sd_missing = list(set(sd_names)-set(correct_files))\n",
    "sd_missing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Orotic acid\n",
      "Taribavirin Hydrochloride\n",
      "LY411575 \n",
      "Chloramphenicol succinate\n",
      "656 657\n"
     ]
    }
   ],
   "source": [
    "not_found = []\n",
    "name_changes = {'Orotic-acid': 'Orotic acid',\n",
    " 'Taribavirin-Hydrochloride': 'Taribavirin Hydrochloride',\n",
    " 'LY411575':'LY411575 ',\n",
    " 'Chloramphenicol-succinate':'Chloramphenicol succinate'}\n",
    "\n",
    "source_folder = os.path.join(data_dir, \"original\", 'SD')\n",
    "destination_folder = os.path.join(data_dir, \"original\", 'SD_curated')\n",
    "for k,v in name_changes.items():\n",
    "    for file_name in os.listdir(source_folder):\n",
    "        base_name = os.path.splitext(file_name)[0]\n",
    "        if base_name in v:\n",
    "            print(base_name)\n",
    "            source_path = os.path.join(source_folder, file_name)\n",
    "            destination_path = os.path.join(destination_folder, file_name)\n",
    "            shutil.copy2(source_path, destination_path)\n",
    "    num_files = len(os.listdir(destination_folder))\n",
    "\n",
    "print(len(sd_names), num_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Duplicate files found:\n",
      "Base name: SA5_0007 - Files: SA5_0007.sdf, SA5_0007.mol\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from collections import defaultdict\n",
    "directory_path = os.path.join(data_dir, \"original\", 'SD_curated')\n",
    "file_dict = defaultdict(list)\n",
    "for filename in os.listdir(directory_path):\n",
    "    base_name = filename.replace('.mol', '').replace('.sdf', '').replace('.mol2', '')\n",
    "    file_dict[base_name].append(filename)\n",
    "duplicates = {key: value for key, value in file_dict.items() if len(value) > 1}\n",
    "if duplicates:\n",
    "    print(\"Duplicate files found:\")\n",
    "    for base_name, files in duplicates.items():\n",
    "        print(f\"Base name: {base_name} - Files: {', '.join(files)}\")\n",
    "else:\n",
    "    print(\"No duplicates found.\")\n",
    "#manually delete the .sdf file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "626 656\n"
     ]
    }
   ],
   "source": [
    "# FINAL Check\n",
    "np_files = len(os.listdir(os.path.join(data_dir, \"original\", 'NP_curated')))\n",
    "sd_files = len(os.listdir(os.path.join(data_dir, \"original\", 'SD_curated')))\n",
    "\n",
    "print(np_files, sd_files)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Curated list standardised\n",
    "\n",
    "From the NP_curated and SD_curated we try to obtain only molecules that can be parsed by the standardiser (see scripts/00_parse_manually_curated_data.py)\n",
    "We also keep the molecules that:\n",
    "- Cannot be parsed by the standardiser\n",
    "- Are duplicated (same molecule different stereochemistry, which is not taken into account at 2D level)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def molecule_loader(subfolder):\n",
    "    sdf_paths = []\n",
    "\n",
    "    for fn in os.listdir(subfolder):\n",
    "        sdf_paths.append(os.path.join(subfolder, fn))\n",
    "\n",
    "    mols = []\n",
    "    paths = []\n",
    "    names = []\n",
    "    for sdf_path in sdf_paths:\n",
    "        name = sdf_path.split(\"/\")[-1][:-4]\n",
    "        suppl = rdkit.Chem.SDMolSupplier(sdf_path)\n",
    "        mols_ = [mol for mol in suppl if mol is not None]\n",
    "        if len(mols_) == 0:\n",
    "            continue\n",
    "        if len(mols_) > 1:\n",
    "            mols_ = [mols_[0]]\n",
    "        mols += mols_\n",
    "        paths += [sdf_path]\n",
    "        names += [name]\n",
    "\n",
    "    assert len(mols) == len(names)\n",
    "    print(\"TOTAL MOLS\", len(mols))\n",
    "    mols_ = []\n",
    "    non_parsed_mols = []\n",
    "    c = 0\n",
    "    for i, mol in enumerate(mols):\n",
    "        try:\n",
    "            mol = standardise.run(mol)\n",
    "            if mol is not None:\n",
    "                mols_ += [(names[i], mol)]\n",
    "        except:\n",
    "            c += 1\n",
    "            non_parsed_mols += [names[i]]\n",
    "            continue\n",
    "    print(\n",
    "        \"Number of non-standardized molecules (skipped) {0}. File: {1}\".format(\n",
    "            c, subfolder\n",
    "        )\n",
    "    )\n",
    "    return mols_, non_parsed_mols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TOTAL MOLS 18\n",
      "Number of non-standardized molecules (skipped) 18. File: ../data/original/pubchem_sdfs\n"
     ]
    }
   ],
   "source": [
    "mols, non_parsed_mols = molecule_loader(\"../data/original/pubchem_sdfs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final Dataset\n",
    "\n",
    "After checking all molecules that could not be standardised initially (checking the original files from PubChem), we have decided to discard them. We will prepare a NP and SD folder with only the files that we process in all_molecules.csv to do the MOE analysis and Scaffold analysis only in these molecules, to be homogeneous "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../data/all_molecules.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "616 616 616\n"
     ]
    }
   ],
   "source": [
    "np_names = df[df[\"category\"]==\"natural\"][\"file_name\"].tolist()\n",
    "source_folder = os.path.join(data_dir, \"original\", 'NP_curated')\n",
    "destination_folder = os.path.join(data_dir, \"original\", 'NP_final')\n",
    "correct_files = []\n",
    "for file_name in os.listdir(source_folder):\n",
    "    base_name = os.path.splitext(file_name)[0]\n",
    "    if base_name in np_names:\n",
    "        correct_files.append(base_name)\n",
    "        source_path = os.path.join(source_folder, file_name)\n",
    "        destination_path = os.path.join(destination_folder, file_name)\n",
    "        shutil.copy2(source_path, destination_path)\n",
    "num_files = len(os.listdir(destination_folder))\n",
    "\n",
    "print(len(np_names),len(correct_files), num_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "615 615 615\n"
     ]
    }
   ],
   "source": [
    "sd_names = df[df[\"category\"]==\"synthetic\"][\"file_name\"].tolist()\n",
    "source_folder = os.path.join(data_dir, \"original\", 'SD_curated')\n",
    "destination_folder = os.path.join(data_dir, \"original\", 'SD_final')\n",
    "correct_files = []\n",
    "for file_name in os.listdir(source_folder):\n",
    "    base_name = os.path.splitext(file_name)[0]\n",
    "    if base_name in sd_names:\n",
    "        correct_files.append(base_name)\n",
    "        source_path = os.path.join(source_folder, file_name)\n",
    "        destination_path = os.path.join(destination_folder, file_name)\n",
    "        shutil.copy2(source_path, destination_path)\n",
    "num_files = len(os.listdir(destination_folder))\n",
    "\n",
    "print(len(sd_names),len(correct_files), num_files)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "chempfn-paper",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
